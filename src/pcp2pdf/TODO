- Cluster together within a table all the metrics that share an indom
  so either a table that shows all the metrics shared by the same indom.
  Like: eth0: network.interface.in.bytes, network.interface.mtu, ...
  Need to see how other tools con do it (given the staticness of pdf)
  and related:
  How to visualize stuff with a truckload of instances like the case
  when we collect proc.* with the proc pmda
  When there are a bunch of instances (> X) we could automatically split the
  pages, and still allowing the user to specify the threshold.
  We could also add a switch --group-by-instance and put all the instances
  with the same name on the same page (although not sure how much sense
  that would make)

- Marko mentioned that other tools expect the archive name without the ending number
  i.e. -a ./20141002

- Marko mentioned it hangs on RHEL6 with THREADED=yes
  Either I pinpoint the problematic package (python, matplotlib, xcb) or
  we disable THREADED when using python2.6?

- Split in multitple pages when there are too many graphs
  To fix this get_all_graphs() needs to be rewritten. Now it returns
  all_graphs = [(label, fname, (m0, m1, .., mN), text), ...]
  Need to add the indoms as well so that we can have something like the
  following:
  all_graphs = [(label, fname, {m0:[i0,i1,i2], m0:[i3,i4,i5],
                                m1:[i0,i1,i2], m1:[i3,i4,i5], ...mN), text), ...]

  After that whoever calls the create_graph() function can do a proper job and 
  draw the poper number of indoms

- Stable block device naming. Currently we either put sda,sdb,... or dm-0, dm-1 in
  the indoms for certain pmdas. As neither is stable, could we try to fix
  it by using the wwwid when available?

- Write a custom qa test that creates a pdf with certain metrics (exercising most of the
  options). The test could be running strings on the pdf and see if all the metrics
  names are present (and not the metrics excluded)

- Measure performance of parsing and creating pdf and  down
  the biggest time-consuming culprits. Consider CYTHON for some
  functions?
  The ideal solution would be an api that implements the PcpArchive.get_values()
  straight in C and return a dictionary with all the values

- Make sure it runs from the end install path as well (i.e. install the
  py modules in the proper place) i.e. Fix setup.py and test the packaging

- Currently we need a running pmcd instance to try and fetch help text. 
  Is there a smarter way?

- Work on moving some of the basic archive parsing functionality
  in the pcp python bindings themselves

- Add a default set of custom graphs that are always automatically included

- Verify that all the maths in rate conversion are always correct. Or is there
  a way to disable the rate conversion or do it automatically via PCP libs

- In the progress bar add a % of completion (we know start_time and end_time, so it should be easy)

- Run the code through pylint and pyflake

- Add test cases that exercise all possible command arguments, use some of the
  archive files under qa/

- Have the possibility to use a config file to customize how the pdf looks

- Test it with ex4 archives
  
- Add on ex4 the monitoring and collection of per process data and
  test a pdf creation with that

- Add bash autocompletion

- There is a plan to add a mode to libpcp on archive context creation
  where it will open a directory (like code here wishes to) and produce
  a context which sources data from all archives in that directory.
  When that work is tackled, it may simplify the mode you have here
  (planned?) where it scans a directory for pcp archives.
  [Michele] This needs revisiting when this code appears upstream
