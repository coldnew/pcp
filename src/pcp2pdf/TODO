- Cluster together within a table all the metrics that share an indom
  so either a table that shows all the metrics shared by the same indom.
  Like: eth0: network.interface.in.bytes, network.interface.mtu, ...
  Need to see how other tools con do it (given the staticness of pdf)
  and related:
  How to visualize stuff with a truckload of instances like the case
  when we collect proc.* with the proc pmda
  When there are a bunch of instances (> X) we could automatically split the
  pages, and still allowing the user to specify the threshold.
  We could also add a switch --group-by-instance and put all the instances
  with the same name on the same page (although not sure how much sense
  that would make)

- Override the -a argument and also allow a single file parameter?

- Automatically detect gaps in the PCP archive and mark them as such in the graph

- Measure performance of parsing and creating pdf and jot down
  the biggest time-consuming culprits. Consider CYTHON for some
  functions?

- Make sure it runs from the end install path as well (i.e. install the
  py modules in the proper place) i.e. Fix setup.py and test the packaging

- Add the possibility to add correlation time entries

- Currently we need a running pmcd instance to try and fetch help text. 
  Is there a smarter way?

- Make use of the convenience pmcc.py class

- Work on moving some of the basic archive parsing functionality
  in the pcp python bindings themselves

- Add a default set of custom graphs that are always automatically included

- Verify that all the maths in rate conversion are always correct

- In the progress bar add a % of completion (we know start_time and end_time, so it should be easy)

- Like sarstats make it multiprocess for faster processing ? The archive
  parsing could be done by splitting the whole time interval by the nr of
  the CPUs and each could have an archive reading process.
  The graph creation part is done. We could also parallelize the parsing
  of the archive by having a context per process chewing on intervals split
  by CPU.

- Run the code through pylint and pyflake

- Add test cases that exercise all possible command arguments

- Have the possibility to use a config file to customize how the pdf looks

- Test it with ex4 archives
  
- Add on ex4 the monitoring and collection of per process data and
  test a pdf creation with that

- Add bash autocompletion

- There is a plan to add a mode to libpcp on archive context creation
  where it will open a directory (like code here wishes to) and produce
  a context which sources data from all archives in that directory.
  When that work is tackled, it may simplify the mode you have here
  (planned?) where it scans a directory for pcp archives.
  [Michele] This needs revisiting when this code appears upstream

- Write MAN page
